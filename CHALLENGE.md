# **Challenge: Implementing an API and Database on Kubernetes**

## **Introduction**

In this challenge, you will consolidate the knowledge acquired about Kubernetes, implementing an **application and connecting it to a database** running everything on a Kubernetes cluster. The goal is for you to use resources such as Deployments, Services, ConfigMaps, Persistent Volumes (PV/PVC), and probes (*liveness* and *readiness*), in addition to exploring automatic scaling strategies (*Horizontal Pod Autoscaler - HPA*).

This challenge simulates a real scenario, where you need to configure and manage two main components:

- **Application (API)**: a backend application (e.g.: Node.js, Python, etc.) that performs operations on the database.
- **Database**: a relational database (e.g.: MySQL or PostgreSQL) configured on the cluster.

In the end, you will also implement basic observability to monitor the cluster behavior and verify the integration between the two services.

---

## **Tasks**

1. **Kubernetes Cluster Configuration:**
    - Configure a local Kubernetes cluster using Kind or Minikube.
    - Create a namespace called `challenge-api` to isolate the application resources and a namespace called `challenge-db` for the database resources.
2. **Creating Deployments:**
    - Configure two Deployments:
        - One Deployment for the **API**.
        - One Deployment for the **Database** (e.g. MySQL, PostgreSQL or MongoDB).
    - Both Deployments should have **1 initial replica**, and can be scaled later with HPA.
    - Add *livenessProbe* and *readinessProbe* to the API Deployment to check the health and readiness of the application.
    - In the Database Deployment, configure secrets to define access credentials (user, password, etc.).
3. **Service Configuration:**
    - Create two Services to expose the Deployments:
        - A Service of type `ClusterIP` for the database, allowing the API to connect to it.
        - A Service of type `ClusterIP` for the API, allowing other components in the cluster to communicate with it (optional, if necessary).
4. **Data Persistence with PVC:**
    - Configure a Persistent Volume (PV) and a Persistent Volume Claim (PVC) to store the database data persistently.
    - Mount the PVC on the database Deployment to ensure that the information persists even if the pod is restarted.
    - Make sure the database uses this mounted directory to store its data.
5. **API Configuration for Database Connection:**
    - Use a ConfigMap or Secret to store the connection string or variables related to the database, such as host, port, and database name.
    - Configure the API Deployment to consume this information through environment variables injected by the ConfigMap or Secret.
    - Make sure the API is working correctly and connected to the database.
6. **Automatic Scaling with HPA:**
    - Configure the *Horizontal Pod Autoscaler (HPA)* for the API Deployment, to automatically scale based on CPU utilization. Set a range between **50% and 80% CPU utilization** as the threshold for scaling.
    - Enable the *Metric Server* in the cluster to collect metrics.
7. **Integration Tests:**
    - After configuring the Deployments and Services, create a simple route in the API that allows you to test the integration with the database. Example:
        - A GET endpoint `/status` that checks connectivity to the database and returns "Connection OK".
        - A POST endpoint `/data` that inserts information into the database.
8. **Observability:**
    - Implement a basic observability solution to monitor pods:
        - Use `kubectl logs` to inspect the logs generated by the API and the database.
        - Use `kubectl top pod` to check CPU and memory metrics.
        - (Optional) Configure a monitoring tool to collect performance metrics.
9. **Documentation:**
    - Create a `README.md` file detailing:
        - Step-by-step instructions for configuring the cluster and applying the Kubernetes manifests.
        - Structure of the resources created (Deployments, Services, PV/PVC, ConfigMaps, HPA, etc.).
        - Commands used to check the status of pods, logs, and metrics.
        - Instructions for testing the integration between the API and the database.

---

## **Additional Challenges** *(For those who want to go further!)*

    - Configure a `NodePort` type Service to expose the API outside the cluster and allow access via browser or HTTP client (Postman, cURL, etc.) without using a PortForward.
    - Configure the database with different users and permission levels, ensuring that the API only uses the necessary permissions.
    - Simulate an increase in load on the API to validate the operation of the HPA and record the results observed.
    - Add an endpoint to the API that lists the data stored in the database, showing the persisted information.
    - Configure a *Rolling Update* strategy in the API Deployment to update the version without downtime.

---

## **Expected Results**

At the end of the challenge, you should have:

- Configured and applied Kubernetes resources for an API and a database (Deployments, Services, PV/PVC, ConfigMaps, HPA, etc.).
- Ensured data persistence in the database using PVCs.
- Validated connectivity between the API and the database.
- Implemented probes to validate API health and readiness.
- Understanding auto-scaling with HPA.
- Clear and comprehensive documentation so that anyone can reproduce the project.
